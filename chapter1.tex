\chapter{Introduction}

The problem of selecting the best is to decide the best one within a finite number of alternatives, where the best is defined as the largest or smallest mean, which is inferred from statistical sampling, by real experiments or computer simulations. To solve such kind of problem, the ranking-and-selection(R\&S) procedures are widely adopted. In this thesis, we only focus on the case where the statistical sampling is archived through computer simulations. Consequently, the obtained simulation results are called sample values, and the corresponding R\&S procedures are specifically simulation R\&S procedures.

Not very long ago when serial computing still dominants, simulation R\&S procedures are designed and implemented serially, where the most important performance issue lies on the algorithm complexity, roughly the proportion of the run-time over the problem size. Once the algorithm complexity of the procedure is settled, the time cost will decrease as the serial execution speed increases, without specifically modifying the procedure itself, even the corresponding programming implementation. In this way, a simulation R\&S procedure can easily enjoy the growth of computing power, which relies on faster serial execution speed, as long as the algorithm complexity is satisfying.

However, as we have already entered the age of parallel computing, the computing environment has significantly changed. Due to some physical limitations, the growth of computing power relies more on parallelism, rather than faster serial execution. In other words, the serial execution speed is not growing as fast as it used to be. If those serial simulation R\&S procedures still remain the same, they can hardly utilize the recent development of computing technology, where the parallel computing has already dominated.

On the other hand, the real demand raised from the practical applications of simulation R\&S procedures also requires us to enhance existing procedures. As is pointed out by \cite{ehiorams06ras}, the two-stage Rinott's procedure(see \cite{cistam1978rinott}) is typically applied when the number of alternatives is fewer than 20. As for fully sequential procedure KN(see \cite{tomacs01kn}), the number is considered as no more than 500. Other procedures like NSGS(see \cite{or01nsgs}) which claims to have been designed specifically to solve large-scale R\&S problems, reported 500 alternatives as the largest test case in their paper. It's definitely not enough for many practical scenarios where the number of alternatives is thousands even tens of thousands.

Traditionally, the conflicts between large-scaled problems and limited capacity of simulation R\&S procedures are solved within serial computing environment by optimization via simulation(OvS) algorithms. A recent review can be found in \cite{potwsc09ovs}. Many OvS algorithms guarantee the global convergence, namely deciding the best after spending enough simulation effort. However in most cases there won't be sufficient computing budget for enough simulation effort and those OvS algorithms will stop much earlier than then ideal point, causing the selected alternative losing statistical validity. In other words, serial OvS algorithms will eventually trade statistical validity for computational practicability, which makes it not a perfect replacement for serial simulation R\&S procedures.

On the contrary, within parallel computing environment, the computational tasks are executed simultaneously. A large problem can be divided into smaller ones, and such small problems can be carried out at the same time, thus speeding up the whole process. In typical R\&S scenarios, all the simulation efforts are naturally suitable for divide-and-conquer, and so as for parallel computing.

All the above statement motivates us to re-design and implement simulation R\&S procedures for parallel computing environment, in order to take advantage of recent development of computing technology and gain extra speed-up compared with serial R\&S. At the same time it will also dramatically enlarge the size of the problems that are suitable for applying simulation R\&S procedures, while avoid the statistical validity loss compared with adopting serial OvS.

An important issue worth being noticed here is the scalability of simulation R\&S procedures in parallel computing environment. Scalability refers to the ability of a system to increase its total capability when more resources are added. In our case, it roughly means how much better our simulation R\&S procedures perform when more parallel computing units are provided. Just like the algorithm complexity is adopted to compare serial algorithms, according their growths of running time against the increasing of problem size, thus avoiding the unfairness brought by different hardware configurations, we use the scalability to compare simulation R\&S procedures in parallel computing environment.

With the consideration of scalability, migrating serial simulation R\&S procedures into parallel computing environment becomes challenging. As is pointed out by the Amdahl's Law raised in \cite{amdahl}, once the total workload is fixed, the minimal time cost has a lower bound, which limited by the proportion of the workload which has to be carried out in serial. In general R\&S scenarios, all the workload except from pure repeated simulations are not that easy to get paralleled, which means these workload probably have to be done in serial and may become the bottle neck for the whole simulation R\&S procedure.

Except from the consideration from scalability, other tricky statistical issues also arise. In parallel computing environment, an earlier started task may finish later, since other tasks are also executing simultaneously. Specifically in R\&S scenario, the order of the starting of a sequence of simulation experiments may differ from that of the finishing of the same sequence of simulation experiments. What is worse, the latter order may be totally random. This violates some fundamental assumptions in previous serial simulation R\&S procedures. More discussions can be found in \cite{ras-seq-parallel}, where two new fully sequential simulation R\&S procedures, vector-filling procedure and asymptotic parallel sequential procedure, are proposed to address these statistical problems.

In this thesis we design and implement a general platform for parallel simulation R\&S procedures, with three specific procedures: Rinott's procedure (see \cite{cistam1978rinott}), vector-filling procedure(see \cite{ras-seq-parallel}), asymptotic parallel sequential procedure (see \cite{ras-seq-parallel}), and two specific simulation experiments: $(s, S)$ inventory simulation(see \cite{cissac1985ss}), three stage buffer allocation simulation(see \cite{smoms93threestage}). At any active time, the whole platform consists of three major components, of which the most essential one is a general framework. This framework contains the fundamental work for the parallelism of simulation R\&S procedures, with master-slave architecture in high level, which is a pretty common pattern in software design. The other two are specific procedure and specific simulation experiment, respectively. These two components are implemented as plug-ins of the whole system, no matter the three procedures and two simulation experiments already implemented by us, or any possible procedures and simulation experiments implemented by anyone in the future. In this way, extensibility is archived. Meanwhile, the most important goal we have archived is its high scalability proved in numerical experiments.

Our work is related to three streams. The first one is on R\&S with many literature view papers like \cite{ras-recent-advances}, \cite{ehiorams06ras} and \cite{ms05ras}. Although many types of R\&S procedures exist, in this thesis we only take a frequentist view and consider the formulation of indifference-zone, or IZ for short. IZ is proposed in \cite{toams1954iz} and recent summary can be found in \cite{nyjws95iz}.

The second one is on parallel and distributed simulation (PADS). There exists previous work that use parallel and distributed computing to deal with single extremely long-running simulation in \cite{potwsc05ras}. Fujimoto discuses the synchronization issues when implementing discrete-event simulation in a distributed computing system in \cite{cotacm90fuji}. He also apply cloud computing in \cite{scsmasm10fuji}. These works differ from us that in R\&S scenario it is the huge number of repeated simulations rather than the running time of any single simulation that matters most.

The third one is on parallel computing, or more precisely, our work is based on it. As is pointed out by Moore's law proposed in \cite{moore}, available computing power is becoming more and more sufficient, with a speed of doubling itself every 18 months. It's also applicable in the age of parallel computing. Amdahl's Law \cite{amdahl} figures out that the maximum speed-up we can get from applying parallel computing is limited by the proportion of the workload which has to be carried out in serial. As a consequence, once the total workload is fixed, the minimum time cost to finish the workload has a lower bound. Another optimistic viewpoint is from Gustafson's law \cite{gustafson}, states that if the available time is fixed, the total workload that can be finished within that time period will not have upper bound as long as enough computing resource is put in.

The remainder of this thesis is organized as follows. Chapter 2 covers the related background information, including recent development of simulation R\&S procedures and basic theories and technologies of parallel computing. We describe our work in a top-down manner in chapter 3, with parallelism of each simulation R\&S procedures analyzed and extensive numerical results presented. Comparison acceleration in serial part of R\&S is discussed in chapter 4, followed by conclusions and some future work in chapter 5.

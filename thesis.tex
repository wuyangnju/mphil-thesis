\documentclass[12pt,a4]{report}
%\documentclass[12pt,twoside]{report}

\usepackage{amsmath}
\usepackage{epsfig,amsthm}
\usepackage{amssymb}
%\usepackage{epsfig,amstex,amsthm}

\linespread{1.6}
%\linespread{2}
%\renewcommand{\baselinestretch}{2}

%\setlength{\baselineskip}{20pt}
\setlength{\topmargin}{-0.5cm}
%\setlength{\topmargin}{0cm}
\setlength{\textheight}{23.5cm}
\setlength{\oddsidemargin}{1.5cm}
\setlength{\evensidemargin}{0cm}
\setlength{\textwidth}{14.4cm}
\setlength{\headsep}{0in}
\setlength{\parskip}{.15in}
%\setlength{\parindent}{.5in}
\setlength{\parindent}{0in}


\begin{document}

\pagenumbering{roman}

\addcontentsline{toc}{chapter}{Title Page}
\null\vspace{0.5in}
%\vspace{0.5in}
\begin{center}
{\Large\bf Your thesis Title}
\vspace{2.5cm}

{\large by}
\vspace{0.5cm}

{\large\bf Your name}\normalsize
\vspace{1cm}

A Thesis Submitted to \\
The Hong Kong University of Science and Technology \\
in Partial Fulfillment of the Requirements for \\
the Degree of Master of Philosophy \\
in Mathematics
\vspace{1.5cm}

14 August 2005, Hong Kong
\end{center}
\thispagestyle{empty}
\newpage

\addcontentsline{toc}{chapter}{Authorization Page}
\begin{center}{\Large\bf Authorization}\normalsize
\end{center}
\vspace{0.5cm}

I hereby declare that I am the sole author of the thesis.

\vspace{0.5cm}

I authorize the University of Science and Technology to lend this thesis
to other institutions or individuals for the purpose of scholarly research.

\vspace{0.5cm}

I further authorize the University of Science and Technology to
reproduce the thesis by photocopying or by other means, in total or in
part, at the request of other institutions or individuals for the
purpose of scholarly research.

\vspace{1.5cm}

\begin{center}
\line(1,0){180}
\smallskip

Your name
\end{center}

\newpage

\addcontentsline{toc}{chapter}{Signature Page}
\null\vspace{1.0cm}
\begin{center}
{\Large\bf Your thesis title}
\vspace{1.5cm}

{\large by}\smallskip

{\large\bf Your name}\normalsize

\vspace{1cm}

This is to certify that I have examined the above MPhil thesis \\
and have found that it is complete and satisfactory in all respects, \\
and that any and all revisions required by \\
the thesis examination committee have been made.

\vspace{2.0cm}


\line(1,0){180} \smallskip

Your Thesis Supervisor
\vspace{1.5cm}

\line(1,0){180} \smallskip

Prof J.S. Li, Head of Department
\medskip

Department of Mathematics\medskip

14 August 2005
\end{center}

\newpage

\addcontentsline{toc}{chapter}{Acknowledgments}
\begin{center}{\Large\bf Acknowledgment}\normalsize
\end{center}
\vspace{0.5cm}

I would like to express my deep gratitude to my supervisor,(supervisor name) 
who has given me a lot of advice and kindly support in my research during two years of my MPhil study. 
I would like to thank the Department of Mathematics for
providing me with postgraduate studentship award so I have the valuable opportunity to study here. Lastly, I would like to thank the postgraduate students in
the Department of Mathematics who have given me a lot of help and valuable opinions in different
ways.

\newpage
\addcontentsline{toc}{chapter}{Table of Contents}
\tableofcontents
\listoffigures
\listoftables

\newpage
\addcontentsline{toc}{chapter}{Abstract}
\begin{center}
{\Large\bf Parallel Implementation of Ranking and Selection Procedures}
\vspace{0.5cm}

{\large \bf WU Yang}\normalsize

\medskip

Department of Industrial Engineering and Logistics Management

\end{center}
\vspace{1.5cm}
\centerline{{\bf \large Abstract}}
\vspace{1.5cm}

In this thesis, we develop an alternative approach to calculate things....!!


\newpage
\pagenumbering{arabic}

\chapter{Introduction}

\section{Introduction}

The Hong Kong University of Science and Technology~\cite{C. D. Mee}.
The aim of the research~\cite{S. Tehrani} is to find out how
effective.....

\section{Section 1.2}

This study is to find out in Table 1.1.
\begin{table}[ht]
\begin{center}
\begin{tabular}{|l|l||l|l|}
\hline
\multicolumn{2}{|c||}{Original order} & \multicolumn{2}{c|}{Correct order} \\ \hline
1 & test & 3 & exam \\
2 & test1 & 5 & exam1 \\
3 & test2 & 9 & exam2 \\
4 & test3 & 12 & exam3 \\
5 & test4 & 8 & exam4 \\
6 & test5 & 10 & exam5 \\
7 & test6 & 7 & exam6 \\
8 & test7 & 11 & exam7 \\
9 & test8 & 1 & exam8 \\
10 & test9 & 2 & exam9 \\
11 & test10 & 6 & exam10 \\
12 & test11 & 4 & exam11 \\ \hline
\end{tabular}
\caption{Sequence of words used and their orders}
\end{center}
\end{table}

\section{Section 1.3}

Section 1.3


\chapter{Ranking and Selection Procedures}

\section{Problem of Selecting the Best}

Selecting the best alternative from a finite number of alternatives, where the best is defined as the largest or smallest expectation of some index specified on every alternative, is a common problem in operation research, in the sense that many practical problems can be modelled in this way. Here we will briefly describe two examples.

\subsection{(s, S) Inventory Problem}

One is about setting up reordering point in inventory management. A general model with two parameters s and S works in the way that as long as the inventory level goes below s, then an order with the amount S - s will be send out, after which the inventory level will regain to S. This is the (s, S) inventory model. And in order to apply this model, extra effort is needed to specify s and S in each practical case to minimize the reordering cost.

In this example, both s and S will be limited by practical issues, like S must be less than the capacity of inventory store, s must between zero and S, both s and S must be natural numbers, which is   often the case, thus making them in a limited range. In another word, the number of possible or feasible combinations of s and S is finite. The index we set up here is the reordering cost and the best combination of s and S is defined as the one with the smallest expectation of the reordering cost index. In this way, practical inventory management problem can be regarded as the theoretical problem of selecting the best.

\subsection{Three-stage Buffer Allocation Problem}

The other one is about allocating buffer space in a multi-stage manufacturing line. Let's take a three stage manufacturing line as an instance. In this case, there are three manufacturing units, or stages, in a manufacturing line, and each stage has its own capacity for manufacturing items. Between each continues two stages, say first and second stage, or second and third stage, there exists a buffer space, which also has its own capacity for manufacturing items. In this model, there're five parameters representing capacities in five locations including the three manufacturing units and tow buffers. The manufacturing manager needs to specify these five parameters to archive maximum throughput for the whole manufacturing line.

Just the same as the previous example, the total number of manufacturing items inside this manufacturing line, or the summation of these five parameters, is limited by practical issues. What's more, these parameters also need to be natural numbers in most cases, thus limiting the number of feasible combinations of these five parameters into finite again. The index here is the throughput for the whole manufacturing line and the best combination of these five parameters is defined as the one which makes the largest expectation of throughput index. Again in this way, the practical three-stage or multi-stage buffer allocation problem is modelled into the theoretical problem of selecting the best.

\section{Ranking and Selection}

In the above two examples, the expectation of the indexes we set up for different cases can be estimated by customized simulation experiments, and inside each case, the expectation of the index for each individual alternative, which is distinguished from other alternatives by a combination of parameters, or a configuration, can be sampled as the output from a single replication of repeated simulation experiments with corresponding configuration as its input. With selecting the best according to the expectations estimated from replicated simulation experiments, this type of solution is often known as a ranking-and-selection, or R\&S, in simulation area.

During the past, many R\&S solutions, or procedures, have been developed. Typically, they allocate simulation effort to all alternatives, namely, running simulation experiments repeatedly for a specified finite number of times against all the configurations, each of which stands for a distinguished alternative. In this way, certain statistical properties are guaranteed when making selection decision, such as the pre-specified probability of correct selection, or PCS for short.

Although many other types of R\&S procedures exist, in this thesis only several representative ones get involved to illustrate our idea. We only take a frequentistâ€™s view and consider the formulation of indifference-zone, or IZ for short. We will briefly review two classic R\&S procedures later in this section, while introduce two newly developed ones in later part of this thesis.

\subsection{Rinott's Procedure}

Rinott's procedure is a classic R\&S procedure developed in 1978 by Rinott. It guarantees with the confidence level $1 - \alpha$ that the alternative get selected is at least $\delta$ better than the second best one. If alternatives within $\delta$ worse than the best one exist, then Rinott's procedure guarantees to select one of these "good enough" alternatives with the same probability. The $\delta$ here is called indifference-zone. It is set by procedure user to determine the smallest difference that is worth getting noticed. In other word, differences less than this $\delta$ can be ignored from a practical viewpoint.

Suppose there are $k$ systems where $k \geqslant 2$. Let $X_{ij}$ denote the $j$th independent sample value from simulation experiments of alternative $i$. We assume that $X_{ij} \backsim N(\mu_i, \theta_i^2)$, where both $\mu_i$ and $\theta_i^2$ are unknown, and each $X_{ij}$ is independent from each other. Also let $\bar{X_i}(r) = r^{-1}\sum_{j=1}^r$, namely the sample mean of first $r$ sample values of alternative $i$, and let $S_i^2 = \frac{1}{n_0 - 1}\sum_{j=1}{n_0}(X_{ij} - \bar{X_i}(n_0))^2$, the sample variance of first $r$ sample values of alternative $i$.

With these symbols, we organize the Rinott's procedure as following steps:

\begin{enumerate}
\item{Set up: } Set up parameters $\alpha$, $\delta$ and $n_0$. $n_0$ is the sample size in first stage and $n_0 \geqslant 2$.
\item{Initialize: } Calculate Rinott's constant $h = h(n_0, k, 1 - \alpha)$ with method mentioned in \cite{rinott-constant}. Collect $n_0$ independent sample values $X_{ij}$, where $j = 1, 2,...,n_0$, for each alternative $i$, by repeatedly running simulation experiments against that alternative, and for $i = 1, 2,...,k$, calculate $S_i^2$. Let 
$$ N_i = \max\{n_0, \lceil \frac{h^2S_i^2}{\delta^2} \rceil\} $$ where $N_i$ is the number of sample value that will eventually taken from alternative $i$.
\item{Stopping Rule: } If $n_0 \geqslant \max N_i$ then stop and select the alternative with the best sample mean, namely the largest or smallest $\bar{X_i(n_0)}$. Else take $\max\{N_i - n_0, 0\}$ extra sample values from each alternative $i$ by continue repeating the simulation experiments, after which select the alternative with largest or smallest $\bar{X_i(N_i)}$ as the best.
\end{enumerate}

And specifically, if the best alternative is defined as the one with largest expectation, then Rinott's procedure guarantees
$$ Pr\{\text{select alternative i }|\text{ }\mu_i - \mu_j \geqslant \delta \} \geqslant 1 - \alpha $$
where $1/k < 1 - \alpha < 1$. Since this thesis focuses on implementation, reader interested in the proof may refer to \cite{ras-recent-advances}. 

One related issue worth mentioning here is that the total sample size required in Rinott's procedure is slightly less that $O(k\log{k})$, which determines the total computing effort.

\subsection{A Fully Sequential Procedure}

While the Rinott's procedure mentioned above is a classic two-staged procedure, the procedure we will review right after this paragraph is a newly developed procedure featured as fully sequential. It first appears in \cite{ras-seq-jeff} by L. J HONG in 2004. This procedure provides good statistical validity while keeps a great opportunity of decreasing total number of sample values, approximately the total computing effort need to spend, at same time. The key point is that if at any time there is enough evidence to prove that some certain alternative is clearly inferior, then this alternative will get eliminated immediately in theoretic like what most fully sequential procedures do.

With the symbols same as those in Rinott's procedure, we organize the this procedure as following steps:

\begin{enumerate}
\item{Set up: } Set up parameters $\alpha$, $\delta$ and $n_0$. Let $\lambda = \eta / 2$ and $a$ be the solution of the equation below:
$$ E[\frac{1}{2}\exp(- \frac{a\delta}{n_0 - 1} \Psi)] = 1 - (1 - \alpha)^{\frac{1}{k - 1}} $$,
where $\Psi$ is a random variable with PDF
$$ f_{\Psi}(x) = 2 [1 - F_{\chi_{n_0 - 1}^2}(x)] f_{\chi_{n_0 - 1}}^2(x) $$,
and $F_{\chi_{n_0 - 1}^2}(x)$ and $f_{\chi_{n_0 - 1}}^2(x)$ are the CDF and PDF of $\chi^2$ distribution with $n_0 - 1$ degrees of freedom.
\item{Initialize: } Let $I = {1, 2,...,k}$ be the set of current surviving alternatives. Again collect $n_0$ independent sample values $X_{ij}$ where $j = 1, 2,...,n_0$ and $i = 1, 2,...,k$ by repeating simulation experiments against corresponding alternatives. Let $r$ be the counter of total number of samples and $n_i(r)$ be the number of samples of alternative $i$. Till now, $r = kn_0$, and $n_1(r) = n_2(r) =Â·Â·Â·= n_k(r) = n_0$.
\item{Screening: } For all $i,j \in I$ and $i \neq j$, let 
$$ \tau_{ij}(r) = [\frac{S_i^2}{n_i(r)} + \frac{S_j^2}{n_j(r)}]^{-1} $$
and
$$ Y_{ij}(\tau_{ij}(r)) = \tau_{ij}(r)[\bar{X_i}(n_i(r)) - \bar{X_j}(n_j(r))] $$.
Let $I^{old} = I$ and 
$$ I = I^{old}\\\{i \in I^{old}: Y_{ij}(\tau_{ij}(r)) < \min[0, - a + \lambda \tau_{ij}(r)] \text{ for some } j \in I^{old} \text{ and } j \neq i\} $$
\item{Stopping Rule: } If $|I| = 1$ then stop and select the only alternative in $I$ as the best. Else let $r = r + 1$, take the $r$th sample by running simulation experiment, update $n_i(r)$ for all $i \in I$, and goto \textbf{Screening}.
\end{enumerate}

A missing specification of the above procedure lays in the part of stopping rule, namely the decision of which alternative is the next sample value from, or in other words, which configuration should be used in next replication of simulation experiment. This decision making rule is called sampling rule.

In \cite{ras-seq-jeff}, such sampling rule is applied: after first stage, run next simulation experiment against the surviving alternative with the lowest $n_i(r)/Si$. If a tie exists, run next simulation against the surviving alternative with lowest $S_i$ among them.

The sample size of such fully sequential procedures is typically smaller than that in Ronott's procedure except for some extreme case. Interested reader may refer to \cite{ras-seq-jeff} for detailed properties as well statistical validity proofs.

\section{Shortcoming of Scalability}

Although existing R\&S procedures have gained great achievement in theoretical areas like statistics, practically these procedures can hardly handle scenarios where the number of alternatives is huge. As pointed out by Kim and Nelson (2006a), the two-stage Rinottâ€™s procedure is typically applied when the number of alternatives is fewer than 20. As for fully sequential procedure KN, the number is considered as no more than 500. Other procedures like NSGS which claims to have been designed specifically to solve large-scale R\&S problems, reported 500 alternatives as the largest test case in their paper. However in practice, it is common to have thousands even tens of thousands of alternatives needed to be considered, which has exposed the shortcoming of scalability in R\&S.

\subsection{A Theoretical Make Up}

Traditionally, the shortcoming of scalability in R\&S are made up through solutions of optimization-via-simulation, or OvS for short. OvS is a kind of solutions, or algorithms, which also aims at the problem of selecting the best. It guarantees global convergence, namely when simulation effort goes to infinity, it can eventually guarantee the correct selection of best.

However, the infinite simulation effort means exactly the R\&S procedures. In most of the cases, OvS algorithms will stop so early at some point that it is still quite far way from infinity, which consequently breaks the statistical guarantee of optimal selection. What is worse, the result give by the OvS algorithm may be significantly inferior to the optimal one, making it useless to the selecting of the best problem. 

Essentially speaking, any OvS algorithm will sacrifice statistical guarantee for time-saving through partially applying R\&S. Therefore, it is only a make up solution with trade-off between statistical guarantee and time cost.

\subsection{A Possible Way Out from Implementation}

Since trading off between statistical guarantee and time cost looks like a zero-sum game, we do hope to find a way that can deal with practical-sized problem of selecting the best, with both fully applying R\&S procedures to archive statistical guarantee, and decreasing the time cost to a reasonable amount so that we can afford to wait. A possible way out is from implementation.

As is pointed out by Moore's law, available computing powerful is becoming more and more sufficient, with a speed of doubling itself every 18 months. But fully taking advantage of the fast-growing computing power nowadays is not as straightforward as it used to be, since the time of serial computing, when the growth of computing power relays on serial execution speed, has already past, and we're in the age of parallel computing.

\chapter{Parallel Computing}

Parallel computing means executing computational tasks simultaneously. It is useful when a large problem can be divided into smaller ones, and such small problems can be carried out at the same time, thus speeding up the whole process.

\section{Different Parallelism Levels}

In terms of granularity, parallel computing can be divided into three different levels: bit level, instruction level and task level.

\subsection{Bit-Level Parallelism}

Bit-level parallelism is related to the size of computer word. Inside the processor, several bits are manipulated together, which composes a computer word, and the size of computer word is defined as the number of bits it contains. Historically, 4-bit computer word has been replaced by 8-bit, then 16-bit, 32-bit, until today's 64-bit. Although not very close to ordinary people, computer with 128-bit computer word or even larger one has been made.

\subsection{Instruction-Level Parallelism}

Instruction-level parallelism is a more interesting topic in computer architecture. An instruction is a computer word with special meaning for processors. Any computer program will essentially become a stream of instructions before getting executed, no matter what programming language it is written in. These instructions will get re-ordered and grouped before executing, certainly without changing the result of original instruction stream. Besides, an instruction can be divided into several stages, with each stage corresponding to a certain action, such as instruction fetch, decode, execute, memory access, write back and so on, which enables the instruction pipeline in modern processors. In addition to pipeline, the pipeline units inside some modern processors can even handle more than one action at a time, which is known as super-scalar.

Before instruction-level parallelism is adopted, the instructions inside a stream are executed serially. At any time, there is at most one instruction getting executed. The next instruction won't start unless the previous one finishes. Under such circumstances, the execution time is easy to calculate, which equals the average execution time per instruction, multiplied by the number of instructions. Without changing the amount of instructions inside a stream, the only way to decrease the execution time is to increase CPU clock frequency, which is called frequency scaling.

However, frequency scaling is limited by many physical constraints, like power consumption, heat generation and so on. Thus instruction-level parallelism has become a way out under current physical constraints, and attracts many researchers in the field of computer architecture.

\subsection{Task-Level Parallelism}

Task-level parallelism is a higher level parallelism in the sense that it needs support from both hardware and software. In another word, both bit-level parallelism and instruction-level parallelism are implemented inside processors, making them transparent, or invisible to software. Even operating system, the most fundamental software inside computer, do not have control on bit-level and instruction-level parallelism. 

Earlier computers without task-level parallelism can also support multi-tasking, since multi-tasking operating system takes care of switching different tasks back and force from time to time according to some strategy, including saving and restoring context for each task, and does not have to wait until current task finishes. From the viewpoint of a user, the computer is doing task in parallel, but precisely speaking, such fake parallelism should be called concurrency. 

As for modern computers with task-level parallelism support, like multi-core or multi-processor, which will be introduced later, they do have the ability to make different tasks executing in real parallel, certainly also with the support from software. We will look into the details of how hardware and software support parallel in the next two sections.

\section{Hardware Parallel Support}

\subsection{Multi-core Processor}

Computers with Von Neumann architecture, which is still the dominant computer architecture today, will have at least one processor inside. For a processor, it can contain more than one execution units, or cores, on the same chip. Such processor is known as multi-core processor. Different cores inside the same multi-core processor can handle different instruction streams at the same time, thus achieve the real parallel. By the way, a core can also be super-scalar, thus handles multiple instructions from the same instruction stream. As for Intel's Hyper-Threading technology, which uses the same core to handle a different instruction stream when this core get into idle for some reason, is a form of pseudo multi-core processing.

\subsection{Multi-processor Computer}

One step further from multi-core processor, a computer can have more than one processors, like SMP, standing for symmetric multi-processor, which is a single computer equipped with multiple identical processors. The existence of multiple processors inside the same computer brings about the issue of accessing memory and communicating with each other.

A computer where every address of memory can be accessed within equal latency and bandwidth is recognized as Uniform Memory Access(UMA), otherwise it belongs to Non-Uniform Memory Access (NUMA). SMP is one kind of Uniform Memory Access, while MPP, standing for massively parallel processor, which is a single computer with multiple networked processors, belongs to Non-Uniform Memory Access.

Communication among processors of the same computer can be implemented in many ways, from a simple shared media like memory or bus to complicated internal network involving topologies like star or ring and routing strategies if some processors are not connected directly.

\subsection{Cluster}

A group of standalone computers can be connected via network to compose a cluster. Since these connected computers work together, this cluster can be regarded as one powerful computer. A cluster composed by multiple identical computers connected via TCP/IP Ethernet local area network is called Beowulf cluster, which is the most common type of cluster.

Computers composing a cluster does not have to be identical with each other, which can increase the flexibility, but also increase the difficulty for load balancing. Except from the extra network compared to single computer, other technical issues like configuration, monitoring even fault tolerance also create challenges that can't be ignored.

\subsection{Distributed Computing}

In a narrow sense, distributed computing means using computers communicating over the Internet to work together on a certain problem. Comparing with a cluster, communication cost over the Internet is much higher since the throughput is low while the latency may be unstable. So typically such distributed computing only deals with embarrassingly parallel problems in which communications among sub-problems are not that frequent. SETI@home is one of the best-known example of such distributed computing.

In a wider sense, distributed computing means multiple computational entities, or nodes, with their own local memory and communicating by message passing, working together to archive a certain goal. If these nodes are deployed on different physical machines in a cluster, they possess their own local physical memory and pass messages through network protocols. On the contrary, if they're deployed on the same physical machine, then the nodes are degenerated to local processes and possess their own virtual memory only. Processes on the same physical machine can certainly pass messages through network protocols via local loop network as if they're on different physical machines, but they can also choose to communicate through local operating system as a more efficient choice. For the concept of process, we will introduce it in the next paragraph.

\section{Software Parallel Support}

\subsection{Process upon Operating System}

Process is a fundamental concept in operating system. In section 3.1.3, we introduced how multi-tasking operating system make multiple tasks executed concurrently without hardware task-level parallelism. Here, the tasks should be called processes. Even with the hardware task-level parallelism, modern operating systems also regard different tasks as different processes, and provide basic services like scheduling execution on multiple cores or processors, pass messages if processes need communication, thus taking advantage of the real parallelism ability from corresponding hardware.

Formally speaking, process is an executing instance of a computer program. A computer program alone contains only static instructions, while a process also contains current execution status. Of course, multiple processes can be different instances from the same program.

With the help of operating system, processes are isolated from each other, in the sense that processes have no idea whether they're sharing CPU, memory or other computing resources with other processes, as if they occupy the whole computer, although actually the CPU is time division multiplexed while memory is space division multiplexed. Nevertheless, in this way programmers can focus on one program without considering other processes on the same machine.

On the other hand, processes on the same physical machine can communicate with each other through mechanisms provided by operating system, like shared memory, pipeline, where the output of one process get passed to another as input, and so on.

Achieving parallelism through multiple processes is based on the underlay distributed memory architecture, even they're on the same physical machine, since the operating system isolated the memories by virtual memory mechanise. Another famous memory architecture is called shared memory architecture, and we will use the concept of thread to illustrate it in the next paragraph.

\subsection{Thread inside Process}

Thread is a lighter concept compared with process in the sense that it is always contained inside a process. A process can have multiple threads, but at least have one, the "main" thread. Scheduling and cooperating multiple threads inside the same process involves less about operating system, thus saving computing resources. Besides, the underlay shared memory architecture, covering the memory spaces of containing process, has also provided more convenience and flexibility for programmers.

Except from thread, or precisely the POSIX Thread, OpenMP is another most widely used shared memory API. API is short for Application Programming Interfaces and can be regarded as the conventional way to use libraries installed on or provided by operating system. Both two APIs are supposed to be provided on different operating systems, in another word, cross-platform. However, the API related to process will be different if they're on different kind of operating systems since its highly coupled with underlay operating system and not cross-platform.

\subsection{Communication through Protocol}

The only way for processes deployed on different physical machines to communicate is via network, thus network communication protocols are involved. In other words, network protocols are involved within distributed computing. Network protocol is a set of rules for exchanging messages among computers. No matter whether communicating computers are identical or not, they can communicate with each other as long as they use the same network communication protocol.

Network communication protocol is layered. The theoretical standard is OSI model with seven layers, while the practical standard is TCP/IP model, which only has for broad layers: link layer, Internet layer, transport layer and application layer.

For software developers of distributed application, including pure parallel computing, the design decision starts varying from the Transport layer, with basically two big choices: Transmission Control Protocol(TCP) or User Datagram Protocol(UDP). TCP provides reliable, ordered, error-checked delivery of data, while UDP is connectionless and emphasizes more on lowered overhead and reduced latency. In other words, using UDP brings the chance to speed up communication with the risk of possible invalid transmitted data. Software developers of distributed application can choose to ignore the risk or pay more code on ensuring transmitted data.

Beyond transport layer, software developers also need to make design decision of application layer, say, to use existing protocols like HTTP or develop application specified protocol. HTTP stands for Hypertext Transfer Protocol and is based on client/server model. It is well supported in most dominant programming languages in format of library thus quite easy to use. Developing application specified protocol based on TCP or UDP may gain extra performance but also cost more effect and taking the risk of non-robustness.

\section{Performance and Scalability}

\subsection{Speed-up Ratio}

Speed-up ratio shows how much a parallel program is faster than its corresponding sequential program. It's defined in the following formula:

$$ S_n = \frac{T_1}{T_n} $$

Here $n$ is the number of parallel execution units, $T_1$ is the execution time of the sequential program and $T_2$ is the execution time of the parallel program with n parallel execution units. Linear speed-up or ideal speed-up is archived when $S_n = n$. 

We also defined efficiency as

$$ E_n = \frac{S_n}{n} = \frac{T_1}{nT_n} $$.

It's typically between zero and one, estimating how well the computing power is utilized, or how much cost is brought by communication or other issues. A trivial case is linear speed-up parallel program will always have an efficiency of 1.

\subsection{Amdahl's Law and Gustafson's Law}

Linear speed-up is only an optimally case. Most parallel programs will have a part of the program which is serial, or can not be paralleled at all, and it is this part of serial program will eventually limit the overall speed-up available from parallelism. This is the fact pointed out by Amdahl's law, which defines the speed-up ratio as:

$$ S_n = \frac{1}{(1 - p) + \frac{p}{n}} $$

Here $n$ is the number of parallel execution units, $p$ is the portion of program that can be paralleled, thus $(1-p)$ is the portion of the serial part. As $n$ goes to infinity, we have:

$$ \lim_{n \to +\infty} S_n = \lim_{n \to +\infty} \frac{1}{(1 - p) + \frac{p}{n}} = \frac{1}{1 - p} $$

And this is the maximum speed-up we can archive from parallelism, no matter how well we do in other aspects like scheduling or communication, as long as the portion that can be paralleled is fixed.

Amdahl's Law is pessimistic in the sense that once the computation amount is fixed we can only archive a limited speed-up ratio. However, from another viewpoint of fixing the total execution time instead of total computation amount, we have the optimistic Gustafson's law, which defines the speed-up ratio as:

$$ S_n = (1 - p) + n \times p $$

Here $n$ and $p$ is still the same meaning as what they're in Amdahl's Law but the speed-up ratio does not have a upper limit as $n$ goes to infinity any more.

Be attention that both Amdahl's law and Gustafson's law assume that the execution time of the serial portion of the program is independent of the number of execution units, which always violated in practise, and we will give an example later.

\subsection{More Affecting Issues}

The first issue is dependency, which appears when computing depends upon prior result. These computation must be carried out in order, thus forming a dependence chain and explains how serial portion in a parallel program exists. No program can run more quickly than the longest chain, which is also known as the critical path.

The second one is race condition, which appears when two or more executing units update a certain variable at the same time and causing errors. This is because the two instruction streams can cross with each other in any order. For example, if both executing units would like to read a variable a, plus it by one, then write result back, the actual execution order could be one executing unit read a's value between the other executing unit's read and write back, thus making the final result equals to a's original value plus 1 rather than 2.

The way to solve race condition is using a lock to provide mutual exclusion when accessing variables shared to multiple execution units, thus making the whole process serial, behaving in a deterministic way and guarantee the correctness. However, here the serial portion will be linear to the number of executing units, which violates the assumption of both Amdahl's law and Gustafson's law.

Barrier is a special format lock for synchronization purpose. Basically synchronization here means two or more executing units join up at a certain point, for example, before print out all the final result. Then a barrier is used to stop the instructions after the join point from executing before every executing units reach this join point.

The third issue is deadlock, appears when multiple locks are involved. For example, if executing unit one locked variable a, and tried to lock variable b, while executing unit two locked variable b, and tried to lock variable a, then both executing units will wait for each other to release the mutual exclusion of the variable it is waiting for. This situation is called deadlock and the program will never finished. Some lock-free and wait-free algorithms can avoid the usage of lock or barrier, but they're more difficult to implement and suitable use cases are limited.

There're still many other issues not mentioned here but also need to take consideration before carrying out parallel computing.

\section{Common Parallel Patterns}

A pattern is a tested solution to a certain kind of problem under some conditions. Here we have summarised several common patterns in parallel computing and listed them below.

\subsection{Embarrassingly Parallel}

Embarrassingly parallel looks more like a problem rather than a pattern. It deals with problems where separating the original problem into smaller ones takes very little or no effort at all. This happens when no communication or dependence exists among separated small problems, for example, if all sub-problems just need to do the same operation and they are independent from each other.

Some problems can be turned into embarrassingly parallel easily. For example, if all sub-problems dependent only on global data structure, then just replicate global data to all sub-problems thus being embarrassingly parallel.

\subsection{Pipeline}

Pipeline is a classic pattern or architecture. We have already introduced hardware pipelines inside processor cores in section 3.1.2, and here we will introduce software pipeline pattern or architecture, for parallel purpose.

Software pipeline is composed of several threads or processes, which depends on implementation. A continuous data stream flows into the first thread or process as input and pass corresponding output as input to next thread or process. It continues so that every thread or process can do certain action on part of data simultaneously.

The most popular software pipeline should be Unix pipeline. It composes Unix commands into a pipeline with simple $\mid$ signs, which makes Unix shell extremely strong and flexible. It is implemented with multiple processes.

\subsection{Master-Slave}

Master-Slave pattern or architecture may be the easiest one to implement on distributed environment. In a Master-Slave structure, a master process takes care of the essential serial part of the whole parallel program, while several slaves execute the paralleled workload. Any slave who finishes current workload will inform the master and wait for master's response, like starting a new task.

Since master does serial part and takes extra cost like coordination, it is very easy to become the bottle neck of whole parallel program, although get multi-levelled. We will give a deeper analysis in the next chapter since we adopted this pattern or architecture in our implementation.

Master-Slave pattern is a high-level abstract pattern. There're many specific instances, like fork-join. 

Fork-join maybe the simplest instance of master-slave structure. In this instance the master will start or fork many slaves first, then wait for or join the finish point of all the slaves, with a barrier generally.

Master-Slave structure is so high-level abstract that even the Hadoop map-reduce, which we will introduce in the next section, has a master-slave structured underlay.

\subsection{Map-Reduce}

This pattern has become hot in recent years due to Google's publication \cite{google-map-reduce} followed by Apache's open source implementation named Hadoop. The key idea is to first split original data set into small pieces and perform the same operation called "map" on these data pieces, turning every data piece into key-value pair format. Then perform the same operation called "reduce" on the intermediate key-value pairs and finally combined them together to get final result.

It is much more than a theoretical pattern because of the Hadoop framework. It has covered many parallel implementation details so that programmer can focus on the business logic part, say, "map" operation and "reduce" operation. The only left difficult point for programmers is how to fit original problem into such two steps.

\subsection{Parallel Pattern Conclusion}

Parallel patterns listed above are common and intuitive. They take advantage of data decomposition and functional decomposition, but rely heavily on the natural presentation of original problem and fit problem into suitable patterns.

To expose better parallelism, advanced tricks are involved like Fast Fourier Transform to take advantage of specific domains, which are not that common any more.

Besides, patterns are more art than science. It is not necessary to stick strictly to some pattern as long as the problem get solved.

\chapter{Parallel Implementation of Ranking and Selection Procedure}

\section{Parallelism Analysis of Existing Procedures}

\subsection{Rinott's Procedure}

\subsection{Fully Sequential Procedure}

\section{Revised Procedures for Parallelism}

\subsection{Vector Filling Procedure}

\subsection{Parallel Sequential Procedure}

\section{Architecture Overview}

\subsection{Master-Slave Structure}

\subsection{Master}

\subsection{Slave}

\subsection{Web Interface}

\section{Extension Points for Universality}

\subsection{Specification of Self-customized Procedure}

\subsection{Interfaces for Self-customized Procedure}

\subsection{Specification of Self-customized Simulation}

\section{Internal Features for Performance}

\subsection{Buffered Communication}

\subsection{Heaped Comparison}

\chapter{Numerical Result and Analysis}

\chapter{Conclusions and Future Work}

This thesis has discussed an appropriate index.....!!


\newpage
\addcontentsline{toc}{chapter}{Appendix}
\appendix


\chapter{Appendix chapter}

Here write down your Appendix ...


\newpage
%\nocite{*}
%\bibliographystyle{plain}
\bibliographystyle{ieeetr}
\addcontentsline{toc}{chapter}{Bibliography}
%\bibliography{thesis}

\begin{thebibliography}{99}
\bibitem{google-map-reduce}D. Jeffrey, G. Sanjay. MapReduce: Simplified Data Processing on Large Clusters. Google Inc., 2004.
\bibitem{rinott-constant}Wilcox, Rand R. A Table for Rinott's Selection Procedure. JOURNAL OF QUALITY TECHNOLOGY, April 1984, pp. 97-100.
\bibitem{ras-recent-advances}Seong-Hee Kim, Barry L. Nelson. RECENT ADVANCES IN RANKING AND SELECTION. Winter Simulation Conference, 2007.
\bibitem{ras-seq-jeff}L. J Hong. Fully sequential indifference-zone selection procedures with variance-dependent sampling. Naval Research Logistics, 53:464-476, 2006.
\end{thebibliography}


\end{document}
